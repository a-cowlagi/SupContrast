nohup: ignoring input
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run glorious-plant-9
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/v0bqheoz
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_0_1_2_3_4/wandb/run-20221209_221446-v0bqheoz
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 4289... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▄▅▅▆▇▇▇▇▇▇▇██████████████████
wandb:      train_loss █▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▆▆▇▇▇▇▇███████████████████████
wandb:        val_loss █▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 82.88
wandb:      train_loss 0.50678
wandb:         val_acc 81.8
wandb:        val_loss 0.5249
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced glorious-plant-9: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/v0bqheoz
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_0_1_2_3_4/wandb/run-20221209_221446-v0bqheoz/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run zany-vortex-10
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2x1jm6xh
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_0_1_2_3_4/wandb/run-20221209_221546-2x1jm6xh
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 7603... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▇▇▇▇██████████████████████
wandb:      train_loss █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇▇███████████████████████████
wandb:        val_loss █▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 86.24
wandb:      train_loss 0.40839
wandb:         val_acc 84.2
wandb:        val_loss 0.44624
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced zany-vortex-10: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2x1jm6xh
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_0_1_2_3_4/wandb/run-20221209_221546-2x1jm6xh/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run summer-vortex-11
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/ukhug1es
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_0_1_2_3_4/wandb/run-20221209_221645-ukhug1es
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 10873... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▇▇▇▇██████████████████████
wandb:      train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇▇▇▇█████████████████████████
wandb:        val_loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 85.72
wandb:      train_loss 0.43449
wandb:         val_acc 84.2
wandb:        val_loss 0.43715
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced summer-vortex-11: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/ukhug1es
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_0_1_2_3_4/wandb/run-20221209_221645-ukhug1es/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run fragrant-oath-12
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2k0nt3z0
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_0_1_2_3_4/wandb/run-20221209_221746-2k0nt3z0
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 14537... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▅▆▆▇▇▇▇▇▇▇██████████████████
wandb:      train_loss █▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇▇▇▇▇██▇█████████████████████
wandb:        val_loss █▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 81.12
wandb:      train_loss 0.52453
wandb:         val_acc 79.8
wandb:        val_loss 0.53236
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced fragrant-oath-12: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2k0nt3z0
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_0_1_2_3_4/wandb/run-20221209_221746-2k0nt3z0/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run elated-smoke-13
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/9v7wk63b
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_95_96_97_98_99/wandb/run-20221209_221847-9v7wk63b
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 18108... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▇▇▇▇▇█████████████████████
wandb:      train_loss █▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▆▇▇▇██████████████████████████
wandb:        val_loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 91.04
wandb:      train_loss 0.28059
wandb:         val_acc 88.6
wandb:        val_loss 0.35045
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced elated-smoke-13: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/9v7wk63b
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_95_96_97_98_99/wandb/run-20221209_221847-9v7wk63b/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run fast-snowflake-14
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/29gvk9jo
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_95_96_97_98_99/wandb/run-20221209_221946-29gvk9jo
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 21558... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▆▆▇▇▇▇▇▇█████████████████████
wandb:      train_loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇████████████████████████████
wandb:        val_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 92.52
wandb:      train_loss 0.22427
wandb:         val_acc 90.4
wandb:        val_loss 0.27835
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced fast-snowflake-14: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/29gvk9jo
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_95_96_97_98_99/wandb/run-20221209_221946-29gvk9jo/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run splendid-water-15
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2gp0ekrt
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_95_96_97_98_99/wandb/run-20221209_222045-2gp0ekrt
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 25036... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▆▆▇▇▇▇▇▇█████████████████████
wandb:      train_loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇████████████████████████████
wandb:        val_loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 93.04
wandb:      train_loss 0.22317
wandb:         val_acc 89.8
wandb:        val_loss 0.32481
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced splendid-water-15: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2gp0ekrt
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_95_96_97_98_99/wandb/run-20221209_222045-2gp0ekrt/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run crimson-dream-16
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2nitk0nt
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_95_96_97_98_99/wandb/run-20221209_222145-2nitk0nt
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 28465... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▇▇▇▇▇▇████████████████████
wandb:      train_loss █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇▇▇▇█████████████████████████
wandb:        val_loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 89.96
wandb:      train_loss 0.30456
wandb:         val_acc 87.2
wandb:        val_loss 0.36503
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced crimson-dream-16: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2nitk0nt
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_95_96_97_98_99/wandb/run-20221209_222145-2nitk0nt/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run wise-leaf-17
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/19koz8gt
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_6_11_16_21_26/wandb/run-20221209_222246-19koz8gt
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 31743... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▆▇▇▇▇▇████████████████████
wandb:      train_loss █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▆▇▇▇▇▇████████████████████████
wandb:        val_loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 83.36
wandb:      train_loss 0.49159
wandb:         val_acc 83.4
wandb:        val_loss 0.50405
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced wise-leaf-17: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/19koz8gt
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_6_11_16_21_26/wandb/run-20221209_222246-19koz8gt/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run whole-water-18
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2ye6acm6
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_6_11_16_21_26/wandb/run-20221209_222345-2ye6acm6
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 2891... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▇▇▇▇▇▇▇▇██████████████████
wandb:      train_loss █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇▇▇██████████████████████████
wandb:        val_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 84.96
wandb:      train_loss 0.43228
wandb:         val_acc 82.0
wandb:        val_loss 0.45554
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced whole-water-18: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2ye6acm6
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_6_11_16_21_26/wandb/run-20221209_222345-2ye6acm6/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run worldly-surf-19
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2jqvkly6
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_6_11_16_21_26/wandb/run-20221209_222446-2jqvkly6
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 6167... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▆▇▇▇▇▇████████████████████
wandb:      train_loss █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▆▇▇▇██████████████████████████
wandb:        val_loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 84.56
wandb:      train_loss 0.44199
wandb:         val_acc 83.8
wandb:        val_loss 0.46331
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced worldly-surf-19: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2jqvkly6
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_6_11_16_21_26/wandb/run-20221209_222446-2jqvkly6/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run vague-blaze-20
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2ubhpln3
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_6_11_16_21_26/wandb/run-20221209_222547-2ubhpln3
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 9508... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▄▅▆▆▇▇▇▇▇▇▇██████████████████
wandb:      train_loss █▅▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▅▆▇▇██▇███████████████████████
wandb:        val_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 80.96
wandb:      train_loss 0.53495
wandb:         val_acc 80.8
wandb:        val_loss 0.5267
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced vague-blaze-20: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2ubhpln3
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_6_11_16_21_26/wandb/run-20221209_222547-2ubhpln3/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run twilight-salad-21
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2gnh7vd0
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_56_58_62_66_68/wandb/run-20221209_222647-2gnh7vd0
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 12858... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▇▇▇▇▇██████████████████████
wandb:      train_loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇▇███████████████████████████
wandb:        val_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 94.76
wandb:      train_loss 0.16481
wandb:         val_acc 93.4
wandb:        val_loss 0.195
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced twilight-salad-21: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/2gnh7vd0
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_0/task_56_58_62_66_68/wandb/run-20221209_222647-2gnh7vd0/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run fanciful-vortex-22
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/23k1csea
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_56_58_62_66_68/wandb/run-20221209_222747-23k1csea
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 16144... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▇▇▇▇▇████████████████████████
wandb:      train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁██████████████████████████████
wandb:        val_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 96.32
wandb:      train_loss 0.1226
wandb:         val_acc 94.2
wandb:        val_loss 0.16472
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced fanciful-vortex-22: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/23k1csea
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_10/task_56_58_62_66_68/wandb/run-20221209_222747-23k1csea/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run solar-rain-23
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/1iirlf8m
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_56_58_62_66_68/wandb/run-20221209_222846-1iirlf8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 19501... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▆▆▇▇▇▇███████████████████████
wandb:      train_loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇████████████████████████████
wandb:        val_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 95.88
wandb:      train_loss 0.13901
wandb:         val_acc 93.8
wandb:        val_loss 0.18517
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced solar-rain-23: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/1iirlf8m
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_20/task_56_58_62_66_68/wandb/run-20221209_222846-1iirlf8m/logs/debug.log
wandb: 
wandb: Currently logged in as: 2contrastornot2contrast (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run denim-gorge-24
wandb: ⭐️ View project at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE
wandb: 🚀 View run at https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/3n8xqnz5
wandb: Run data is saved locally in ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_56_58_62_66_68/wandb/run-20221209_222946-3n8xqnz5
wandb: Run `wandb offline` to turn off syncing.
wandb: Waiting for W&B process to finish, PID 22822... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:           epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:   learning_rate ██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_acc ▁▅▆▆▇▇▇▇██████████████████████
wandb:      train_loss █▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         val_acc ▁▇▇▇███████████████████████████
wandb:        val_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:           epoch 30
wandb:   learning_rate 0.0008
wandb:       train_acc 94.44
wandb:      train_loss 0.18803
wandb:         val_acc 92.6
wandb:        val_loss 0.22283
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced denim-gorge-24: https://wandb.ai/2contrastornot2contrast/fine_tuning_SupCE/runs/3n8xqnz5
wandb: Find logs at: ./save/linear_tuning/SupCE/cifar100/supce_on_cifar10_seed_30/task_56_58_62_66_68/wandb/run-20221209_222946-3n8xqnz5/logs/debug.log
wandb: 
